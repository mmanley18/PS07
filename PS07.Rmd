---
title: 'STAT/MATH 495: Problem Set 07'
author: "Meredith Manley"
date: '2017-10-24'
output:
  html_document:
    collapsed: no
    df_print: kable
    smooth_scroll: no
    toc: yes
    toc_depth: 2
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE, warning = FALSE
  )
set.seed(76)

# Load packages
library(ggplot2)
library(tidyverse)
library(broom)
library(knitr)
require(mosaic)
library(ROCR)
library(dplyr)

train <- read_csv("data/cs-training.csv") %>% 
  rename(Id = X1)
test <- read_csv("data/cs-test.csv") %>% 
  rename(Id = X1)
submission <- read_csv("data/sampleEntry.csv")
```

Information on the competition can be found [here](https://www.kaggle.com/c/GiveMeSomeCredit/data).



# Collaboration

Please indicate who you collaborated with on this assignment: 



# Build binary classifier

Build the binary classifier based on a single predictor variable: `DebtRatio`,
`age`, or `MonthlyIncome`. Justify this choice.

### Choose and justify variable selection
```{r}
train2 <- train %>%
  select(DebtRatio, age, MonthlyIncome) %>%
  filter(DebtRatio < 2.5, MonthlyIncome < 20000)

ggplot(train2, aes(x=DebtRatio)) + geom_density()
ggplot(train, aes(x=age)) + geom_density()
ggplot(train2, aes(x=MonthlyIncome)) + geom_density()

# decriptive statistics
favstats(train$DebtRatio)
favstats(train$age)
favstats(train$MonthlyIncome) # many missing values

# model summary
model_formula <- as.formula(SeriousDlqin2yrs~DebtRatio)
model_logisticA <- glm(model_formula, data=train, family="binomial")
summary(model_logisticA)

model_formula <- as.formula(SeriousDlqin2yrs~age)
model_logisticB <- glm(model_formula, data=train, family="binomial")
summary(model_logisticB)

model_formula <- as.formula(SeriousDlqin2yrs~MonthlyIncome)
model_logisticC <- glm(model_formula, data=train, family="binomial")
summary(model_logisticC)
```

> Because `MonthlyIncome` has so many missing values we will not use this variable as the predictor for serious delinquincy in 2 years and because `DebtRatio`did not have a as significant p-value as `age` and `MontlyIncome` in the model summary, we will use `age` as our predictor for this binary classifier.


### Build the Binary Classifier
```{r}
model_formula <- as.formula(SeriousDlqin2yrs~age)
model_logistic <- glm(model_formula, data=train, family="binomial")

# 1.a) Extract regression table in tidy format
model_logistic %>% 
  broom::tidy(conf.int=TRUE)


# 1.b) Extract point-by-point info in tidy format
model_logistic %>% 
  broom::augment() %>% 
  as_tibble() %>% 
  sample_n(5)

# 1.c) Extract summary stats info in tidy format
model_logistic %>% 
  broom::glance()

# fit model and record predictions
model_logistic %>% 
  broom::augment(newdata=test) %>% 
  as_tibble() %>% 
  mutate(p_hat = 1/(1 + exp(-.fitted))) %>%
  sample_n(5)

fitted_model <- model_logistic %>% 
  broom::augment() %>% 
  as_tibble() %>% 
  mutate(p_hat = 1/(1 + exp(-.fitted)))

predictions <- model_logistic %>% 
  broom::augment(newdata=test) %>% 
  mutate(p_hat = 1/(1 + exp(-.fitted)))

train_augmented <- model_logistic %>% 
  broom::augment() %>% 
  as_tibble() %>% 
  mutate(p_hat = 1/(1+exp(-.fitted)))

# Computes the ROC curve
pred <- prediction(predictions = train_augmented$p_hat, labels = train_augmented$SeriousDlqin2yrs)
perf <- performance(pred, "tpr","fpr")

# Computes the Area Under the Curve
auc <- as.numeric(performance(pred,"auc")@y.values)
auc

# select function to create submissions dataset
submission <- predictions %>%
  select(Id, p_hat) 

colnames(submission)[2] <- "Probability"

write.csv(submission, "submission.csv")
```

> By looking at the value for the area under the curve (0.6352512), we see that the predictors in this model do substantially better than the baseline of 0.5. We also checked the AUC for the other models using `DebtRatio` and `MonthlyIncome` as the single predictor and these values were lower than that of `age` so this further supports our decision to use `age` as the single predictor.

# ROC curve

Based on the ultimate classifier you choose, plot a corresponding ROC curve.

```{r}
# This bit of code prints it
plot(perf, main=paste("Area Under the Curve =", round(auc, 3)))
abline(c(0, 1), lty=2)
```



# ROC curve for random guessing

Instead of using any predictor information as you did above, switch your
predictions to random guesses and plot the resulting ROC curve.

```{r}
# Computes the ROC curve based on random guessing
pred <- prediction(predictions = sample(train_augmented$p_hat), labels = train_augmented$SeriousDlqin2yrs)
perf <- performance(pred, "tpr","fpr")

# Computes the Area Under the Curve
auc <- as.numeric(performance(pred,"auc")@y.values)
auc

# This bit of code prints it
plot(perf, main=paste("Area Under the Curve =", round(auc, 3)))
abline(c(0, 1), lty=2)
```

> This plot shows our ROC curve is essentially overlapping the baseline curve, thus showing us that when we compute an ROC curve without using any predictor information that our predictions will not be very accurate.
